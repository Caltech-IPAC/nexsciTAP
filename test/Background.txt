

Background


The nexsciTAP service can be accessed through a number of clients, including
Astroquery/TAPPlus, PyVO, and TopCat.  Trying access with each of these is
important, as this is the way most users will use the service.

However, each has their shortcomings when it comes to bulk testing of the 
service itself.  For the most part, this results from these clients trying 
to make the interface as simple and foolproof as possible (a good thing).

But we as testers prefer to be in direct control of the low-level interactions
and to see the evolution of the job status and results.  This is actually not
that difficult, even using just a browser or web access tools like wget and 
cURL.

Here we describe the process using a couple of bash scripts but this is easily
translated into interaction scenarios in other shells or environments like
PERL and Python.  


----


To initiate a query, we package up the ADQL and send it off using either


   https://exoplanetarchive.ipac.caltech.edu/TAP/sync
      ?query=select+ra,dec,pl_name+from+ps+where+ra+<+4.

or


   https://exoplanetarchive.ipac.caltech.edu/TAP/async?
      query=select+ra,dec,pl_name+from+ps+where+ra+<+4.


The first is a synchronous query and will simply return the data in real 
time (the down side is that queries that take too long will time out and
error off).

The second is asynchronous, which means the server stashes away the
query and returns immediately without actually running it.  Here we have 
URL-encoded the requests (i.e. the plusses instead of blanks) but in both
the browser and using wget the tool will take care of that for you.

The asynchronous query will return a "jobId" handle (random string) that
can be used for further interaction.  With it, you can compose HTTP 
requests to start the query runnings,; determine the status of the query
(still running, finished successfully, or failed), and retrieve a
URL to download the data (or the error message).

So we start the processing, poll until the status says it's done, and
download the results.  If the jobId is "tap_gsc5ifyt", the URLs for the
above functions are


   https://exoplanetarchive.ipac.caltech.edu/TAP/async/tap_gsc5ifyt/phase?PHASE=RUN
   https://exoplanetarchive.ipac.caltech.edu/TAP/async/tap_gsc5ifyt/phase
   https://exoplanetarchive.ipac.caltech.edu/TAP/async/tap_gsc5ifyt/results/result
   https://exoplanetarchive.ipac.caltech.edu/TAP/async/tap_gsc5ifyt/error 


All this can fit in a straightforward bash shell script with functions for
each of the above operations.

This code ends up retrieving the results, but since we implemented it 
primarily for testing we also track other bits of information, like the
status file (XML) maintained on the server and also downloadable.  This
will help us with analyzing any obscure errors.

----

For large scale testing, we want to loop over a large number of queries
and compare the results to a manually vetted copy.  So there is a little
extra code to do this for a single run and a second script to drive the
processing of a list of queries.

This is as close as we get to continuous integration (standard CI is 
difficult since we need an operational web server and back-end DBMS
for all of this).

